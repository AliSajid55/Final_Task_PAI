{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30827,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, AdamW\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.nn.utils import clip_grad_norm_\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.115663Z","iopub.execute_input":"2024-12-28T19:36:21.115997Z","iopub.status.idle":"2024-12-28T19:36:21.120982Z","shell.execute_reply.started":"2024-12-28T19:36:21.115969Z","shell.execute_reply":"2024-12-28T19:36:21.120121Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n        text = self.texts[index]\n        label = self.labels[index]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.float)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.121989Z","iopub.execute_input":"2024-12-28T19:36:21.122226Z","iopub.status.idle":"2024-12-28T19:36:21.142339Z","shell.execute_reply.started":"2024-12-28T19:36:21.122206Z","shell.execute_reply":"2024-12-28T19:36:21.141510Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class BertLSTMClassifier(nn.Module):\n    def __init__(self, bert_model_name, hidden_dim):\n        super(BertLSTMClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.lstm = nn.LSTM(input_size=768, hidden_size=hidden_dim, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, 1)\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():\n            bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        \n        lstm_output, _ = self.lstm(bert_output.last_hidden_state)\n        output = self.fc(lstm_output[:, -1, :])\n        return torch.sigmoid(output).squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.144191Z","iopub.execute_input":"2024-12-28T19:36:21.144391Z","iopub.status.idle":"2024-12-28T19:36:21.159220Z","shell.execute_reply.started":"2024-12-28T19:36:21.144374Z","shell.execute_reply":"2024-12-28T19:36:21.158512Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')  # Replace with your uploaded dataset file name\n\n# Preprocess the dataset\ndf = df.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})  # Map labels to binary\n\ntexts = df['review'].tolist()\nlabels = df['sentiment'].tolist()\n\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.160266Z","iopub.execute_input":"2024-12-28T19:36:21.160482Z","iopub.status.idle":"2024-12-28T19:36:21.765704Z","shell.execute_reply.started":"2024-12-28T19:36:21.160462Z","shell.execute_reply":"2024-12-28T19:36:21.764968Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(50000, 2)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.766438Z","iopub.execute_input":"2024-12-28T19:36:21.766671Z","iopub.status.idle":"2024-12-28T19:36:21.785731Z","shell.execute_reply.started":"2024-12-28T19:36:21.766645Z","shell.execute_reply":"2024-12-28T19:36:21.785152Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"bert_model_name = 'bert-base-uncased'\nmax_len = 128\nhidden_dim = 128\nbatch_size = 32\nepochs = 10\nlearning_rate = 0.001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.786582Z","iopub.execute_input":"2024-12-28T19:36:21.786779Z","iopub.status.idle":"2024-12-28T19:36:21.790730Z","shell.execute_reply.started":"2024-12-28T19:36:21.786761Z","shell.execute_reply":"2024-12-28T19:36:21.789916Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n\ntrain_dataset = TextDataset(train_texts, train_labels, tokenizer, max_len)\nval_dataset = TextDataset(val_texts, val_labels, tokenizer, max_len)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.791419Z","iopub.execute_input":"2024-12-28T19:36:21.791597Z","iopub.status.idle":"2024-12-28T19:36:21.950644Z","shell.execute_reply.started":"2024-12-28T19:36:21.791581Z","shell.execute_reply":"2024-12-28T19:36:21.949869Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Model, Loss, Optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BertLSTMClassifier(bert_model_name, hidden_dim).to(device)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:21.952391Z","iopub.execute_input":"2024-12-28T19:36:21.952617Z","iopub.status.idle":"2024-12-28T19:36:25.247318Z","shell.execute_reply.started":"2024-12-28T19:36:21.952597Z","shell.execute_reply":"2024-12-28T19:36:25.246418Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd1143f342244d3b98a819f775858e4"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"for epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:36:25.248278Z","iopub.execute_input":"2024-12-28T19:36:25.248654Z","iopub.status.idle":"2024-12-28T20:30:35.951143Z","shell.execute_reply.started":"2024-12-28T19:36:25.248632Z","shell.execute_reply":"2024-12-28T20:30:35.950269Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Loss: 0.40683618985414505\nEpoch 2 Loss: 0.3218914108455181\nEpoch 3 Loss: 0.29987481405735017\nEpoch 4 Loss: 0.2824332995772362\nEpoch 5 Loss: 0.269942486089468\nEpoch 6 Loss: 0.25166591953635215\nEpoch 7 Loss: 0.23791376789808275\nEpoch 8 Loss: 0.22052648710906506\nEpoch 9 Loss: 0.20430450218468904\nEpoch 10 Loss: 0.1875486241698265\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"model.eval()\nall_preds = []\nall_labels = []\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        outputs = model(input_ids, attention_mask)\n        preds = (outputs >= 0.5).float()\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:30:35.952047Z","iopub.execute_input":"2024-12-28T20:30:35.952283Z","iopub.status.idle":"2024-12-28T20:31:54.780291Z","shell.execute_reply.started":"2024-12-28T20:30:35.952262Z","shell.execute_reply":"2024-12-28T20:31:54.779604Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"accuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds)\nrecall = recall_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1 Score: {f1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T20:31:54.781001Z","iopub.execute_input":"2024-12-28T20:31:54.781201Z","iopub.status.idle":"2024-12-28T20:31:54.816669Z","shell.execute_reply.started":"2024-12-28T20:31:54.781184Z","shell.execute_reply":"2024-12-28T20:31:54.815891Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.8673\nPrecision: 0.8981047937569677\nRecall: 0.8223764801959984\nF1 Score: 0.8585740168389642\n","output_type":"stream"}],"execution_count":33}]}